import os

import googleapiclient.discovery

def google_api(id):
    # Disable OAuthlib's HTTPS verification when running locally.
    # *DO NOT* leave this option enabled in production.
    # os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    api_service_name = "youtube"
    api_version = "v3"
    DEVELOPER_KEY = "AIzaSyByvn7qNY5UQuE-DbQK2eeYwHTNg2MtSog"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey = DEVELOPER_KEY)

    request = youtube.commentThreads().list(
        part="id,snippet",
        maxResults=300,
        order="relevance",
        videoId= "Mxkg3qLIPC8" #neg lionel winning cup
    )
    
    response = request.execute()

#     print(response)
    return response
response = google_api("621oD2zBSbI")

import pandas as pd
def create_df_author_comments():
    authorname = []
    comments = []
    for i in range(len(response["items"])):
        authorname.append(response["items"][i]["snippet"]["topLevelComment"]["snippet"]["authorDisplayName"])
        comments.append(response["items"][i]["snippet"]["topLevelComment"]["snippet"]["textOriginal"])
    df_1 = pd.DataFrame(comments, index = authorname,columns=["Comments"])
    return df_1 
df_1 = create_df_author_comments()
pd.set_option("display.max_colwidth", None)
pd.set_option('display.max_rows', None)
# df_1

# REMOVE NaN VALUES AND EMPTY STRINGS:
df_1.dropna(inplace=True)

blanks = []  # start with an empty list

for i, comments in df_1.itertuples():  
    if type(comments)==str:            
        if comments.isspace():        
            blanks.append(i)     

df_1.drop(blanks, inplace=True)

import nltk
nltk.download('vader_lexicon') 

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

# df_1= df_1.assign(Score = lambda x: sid.polarity_scores('Comments'))
df_1['Scores'] = df_1['Comments'].apply(lambda x: sid.polarity_scores(x))

#normalize the score
df_1['Compound'] = df_1['Scores'].apply(lambda score_dict: score_dict['compound'])

#label
df_1['Polarity'] = df_1['Compound'].apply(lambda c: 'neutral' if -0.05 < c < 0.05 else ('negative' if c < -0.05 else 'positive'))

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

stop = stopwords.words('english')
df_1['comments (without stopwords)'] = df_1['Comments'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df_1.sort_values(by=['Polarity']).head()

#convert to lower case
lower = lambda x: x.lower()
df_1['comments (without stopwords)'] = df_1['comments (without stopwords)'].apply(lower)  

#remove emojis and special characters
import re
def cleaning_comments(comment):
  comment = re.sub("[🤣|🤭|🤣|😁|🤭|❤️|👍|🏴|😣|😠|💪|🙏|😞|🌺|🌸|🌞|🌻|💐|💓|😥|💔|😪|😑|🏽|😢|😑|😇|💜|🪴|🙌🏻|🇨🇦|🕊|🕯|😭|😔|💙|🏼|✝|🇿]+",'',comment)
  comment = re.sub("[0-9]+","",comment)
  comment = re.sub("[\:|\@|\)|\*|\.|\$|\!|\?|\,|\%|\"|\(|\-|\”|\#|\!|\/|\«|\»|\&|\n|\'|\;|\!|<|>|\'|\’|\\\\]+"," ",comment)
  return comment

df_1['comments (without stopwords)']= df_1['comments (without stopwords)'].apply(cleaning_comments)

# adding score to data frame

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

# df_1= df_1.assign(Score = lambda x: sid.polarity_scores('Comments'))
df_1['scores'] = df_1['comments (without stopwords)'].apply(lambda x: sid.polarity_scores(x))

#normalize the score
df_1['compound'] = df_1['scores'].apply(lambda score_dict: score_dict['compound'])

#label
df_1['polarity (without stopwords)'] = df_1['compound'].apply(lambda c: 'neutral' if -0.05 < c < 0.05 else ('negative' if c < -0.05 else 'positive'))

df_1['Polarity'].value_counts()

df_1['polarity (without stopwords)'].value_counts()

# df_1.loc[(df_1['Polarity'] == 'negative') & (df_1['polarity'] == 'positive')]
df_1.loc[(df_1['Polarity'] != df_1['polarity (without stopwords)'])]
# df_1.loc[(df_1['Polarity'] == 'negative') & (df_1['polarity'] == 'positive')]
df_1.loc[(df_1['Polarity'] != df_1['polarity (without stopwords)'])]

df_1.sort_values(by=['Polarity'])
